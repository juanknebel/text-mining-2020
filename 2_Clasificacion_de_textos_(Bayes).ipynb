{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "2. Clasificacion de textos (Bayes).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jl2rm838yjF3",
        "8SDbY0PTyjGB"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanknebel/text-mining-2020/blob/master/2_Clasificacion_de_textos_(Bayes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-1ien5uyjFP",
        "colab_type": "text"
      },
      "source": [
        "# Clasificaci√≥n de textos: Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSdkEOPJyjFQ",
        "colab_type": "code",
        "outputId": "a7ac486d-5bcb-4e1f-e9c6-3e4711d569e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import movie_reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd73GYAoAYi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews.categories()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAnRgkJAmd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_review_fileids = movie_reviews.fileids('neg')\n",
        "neg_review_fileids[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glkTf_J89sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(movie_reviews.raw(neg_review_fileids[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTiYruP32H7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = []\n",
        " \n",
        "for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        documents.append((movie_reviews.raw(fileid), category))\n",
        "\n",
        "df = pd.DataFrame(documents, columns=['review', 'tag'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3bVVuTtyjFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sample(frac=1,random_state=0).reset_index(drop=True).copy()\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URosYuw_yjFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tag.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBHyIXtLOUuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_split?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPc8EwVeyjFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separamos entre train y test sets\n",
        "X = df.review\n",
        "y = df.tag\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(X,y,stratify=y, test_size=0.20, random_state=1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmRBEf0eyjFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_text.tolist()[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6zzBTrYyjFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limpieza del texto\n",
        "def clean_text(text):\n",
        "  # En este caso particular no hace falta limpiar los textos\n",
        "  return text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aERYr-AhyjFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text):\n",
        "    #return [w for w in word_tokenize(text) if w.isalpha()] # si solo nos interesan palabras\n",
        "    return word_tokenize(text)\n",
        "tokenizer(clean_text(X_train_text.iloc[1]))[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzYVPvRoyjFn",
        "colab_type": "text"
      },
      "source": [
        "## extraccion de Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_YJxrhQvHUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stoplist = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpwupfW1vJ0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stoplist[:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNREn8HNvziD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizo el stoplist con el mismo tokenizar que voy a usar en el corpus\n",
        "stoplist_tokenized = []\n",
        "for w in stoplist:\n",
        "    stoplist_tokenized = stoplist_tokenized + tokenizer(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0m6bsiAwSsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stoplist_tokenized[:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBhscObNyp2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Armo una lista sin repeticiones\n",
        "stoplist_tokenized = list(set(stoplist_tokenized))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsN2X11kyjFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXbPWPJ4yjFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(preprocessor=clean_text, tokenizer=tokenizer, min_df=5,stop_words=stoplist_tokenized)\n",
        "X_train = count_vect.fit_transform(X_train_text) # cuenta frecuencia de tokens y define el diccionario\n",
        "X_test = count_vect.transform(X_test_text) # cuenta frecuencia de tokens existentes en el diccionario\n",
        "X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsFFocLZQy0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udEPCpqOyjFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect.get_feature_names()[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOiwd-yLyjFw",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtGF6wByjFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = MultinomialNB(alpha=1) # alpha es el smoothing parameter\n",
        "scores_cv_nb = cross_val_score(clf,X_train, y_train,cv = 10, scoring='f1_macro')\n",
        "scores_cv_nb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkwvOqBzyjF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"f-score=\",round(scores_cv_nb.mean(),4),\" ( sd =\",round(scores_cv_nb.std(),4),\")\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl2rm838yjF3",
        "colab_type": "text"
      },
      "source": [
        "### Dummy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5QdhS5yjF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_dummy = DummyClassifier()\n",
        "scores_dummy_cv = cross_val_score(clf_dummy,X_train, y_train,cv = 10, scoring='f1_macro')\n",
        "print(\"f-score=\",round(scores_dummy_cv.mean(),4),\" ( sd =\",round(scores_cv_nb.std(),4),\")\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEqeD1EfyjF6",
        "colab_type": "text"
      },
      "source": [
        "## N-gramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVCz6XuPyjF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# incluyo bigramas (aclaracion:si aparece \"best friend\" tambien va a contar para \"best\" y para \"friend\")\n",
        "count_vect = CountVectorizer(preprocessor=clean_text, \n",
        "                             tokenizer=tokenizer, \n",
        "                             min_df=5,\n",
        "                             stop_words=stoplist_tokenized,\n",
        "                             ngram_range=(1,2))\n",
        "X_train_ngrams = count_vect.fit_transform(X_train_text) # cuenta frecuencia de tokens y define el diccionario\n",
        "print(\"numero de features=\",X_train_ngrams.shape[1])\n",
        "\n",
        "scores_cv_ngrams = cross_val_score(clf,X_train_ngrams, y_train,cv = 10, scoring='f1_macro')\n",
        "print(\"f-score=\",round(scores_cv_ngrams.mean(),4),\" ( sd =\",round(scores_cv_ngrams.std(),4),\")\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rPCNpg1yjF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect.get_feature_names()[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SDbY0PTyjGB",
        "colab_type": "text"
      },
      "source": [
        "### Pruebo otros parametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrWSyqreyjGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# incluyo bigramas (aclaracion:si aparece \"best friend\" tambien va a contar para \"best\" y para \"friend\")\n",
        "count_vect = CountVectorizer(preprocessor=clean_text, \n",
        "                             tokenizer=tokenizer, \n",
        "                             min_df=30, \n",
        "                             stop_words=stoplist_tokenized,\n",
        "                             ngram_range=(1,2))\n",
        "X_train_ngrams = count_vect.fit_transform(X_train_text) # cuenta frecuencia de tokens y define el diccionario\n",
        "print(\"numero de features =\",X_train_ngrams.shape[1])\n",
        "\n",
        "clf = MultinomialNB(alpha=1)\n",
        "scores_cv_ngrams_v2 = cross_val_score(clf,X_train_ngrams, y_train,cv = 10, scoring='f1_macro')\n",
        "print(\"f-score =\",round(scores_cv_ngrams_v2.mean(),4),\" ( sd =\",round(scores_cv_ngrams_v2.std(),4),\")\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYdfC9aJyjGG",
        "colab_type": "text"
      },
      "source": [
        "## Selecciono el mejor modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohvoqk2eRaZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series({'NB (mindf=5)':round(scores_cv_nb.mean(),3),'NB with ngrams (mindf=5)':round(scores_cv_ngrams.mean(),3),'NB with ngrams (mindf=30)':round(scores_cv_ngrams_v2.mean(),3)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4br-uv8yjGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(preprocessor=clean_text, tokenizer=tokenizer, min_df=5,stop_words=\"english\",ngram_range=(1,1))\n",
        "X_train_ngrams = count_vect.fit_transform(X_train_text) # cuenta frecuencia de tokens y define el diccionario\n",
        "X_test_ngrams = count_vect.transform(X_test_text) # cuenta frecuencia de tokens existentes en el diccionario\n",
        "print(\"numero de features=\",X_train_ngrams.shape[1])\n",
        "# entreno el clasificador\n",
        "clf = MultinomialNB(alpha=1).fit(X_train_ngrams, y_train)\n",
        "# predigo en el set de testeo\n",
        "y_pred = clf.predict(X_test_ngrams)\n",
        "scores_ngrams = pd.Series(precision_recall_fscore_support(y_test, y_pred,average=\"macro\")[:3],index=[\"precision\",\"recall\",\"fscore\"])\n",
        "print(\"matriz de confusi√≥n\\n\",confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nNgrams\\n\")\n",
        "print(scores_ngrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoR31oK5yjGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3qN7CpyjGR",
        "colab_type": "text"
      },
      "source": [
        "##  Normalizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu6ZdMwKyjGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemizacion: lleva una palabra a su ra√≠z (la cual puede no ser una palabra) \n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stemmer.stem(\"explanation\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWVZLcMnyjGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[(w,stemmer.stem(w)) for w in word_tokenize(clean_text(X_train_text.iloc[0])) if w.isalpha()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0trRgkyjGZ",
        "colab_type": "text"
      },
      "source": [
        "#### notar que \"empathy\" y \"empathize\" no van a la misma raiz!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJW0IvzyjGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stemmer.stem(\"empathy\"))\n",
        "print(stemmer.stem(\"empathize\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RZoFoVyyjGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lematizacion: lleva una palabra a su forma can√≥nica\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRvhh5kZyjGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# para lemmatizar es necesario dar la categoria gramatical(part-of-speach)\n",
        "# los POS validos son (\"a\",\"v\",\"n\",\"r\")(adjetivo,verbo,sustantivo,advervio)\n",
        "wnl.lemmatize(word=\"was\",pos=\"v\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhel3gblyjGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postags = pos_tag(word_tokenize(clean_text(X_train_text.iloc[1])))\n",
        "postags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9OmeWqtyjGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_pos_to_lemma(word,pos,wnl):\n",
        "    if pos.startswith('J'):\n",
        "        return wnl.lemmatize(word,wordnet.ADJ) # adjetivo\n",
        "    elif pos.startswith('V'):\n",
        "        return wnl.lemmatize(word,wordnet.VERB) # vervo\n",
        "    elif pos.startswith('N'):\n",
        "        return wnl.lemmatize(word,wordnet.NOUN)# sustantivo\n",
        "    elif pos.startswith('R'):\n",
        "        return wnl.lemmatize(word,wordnet.ADV) # advervio\n",
        "    else:\n",
        "        return wnl.lemmatize(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LHvNNGayjGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[(w,word_pos_to_lemma(w,pos,wnl) ) for w,pos in postags if w.isalpha()][:25]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0o20WuRgzHz",
        "colab_type": "text"
      },
      "source": [
        "## Zipf's Law"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t0jdzUUffRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Serie con todas las palabras del corpus\n",
        "words_series = pd.Series([word for rev in df.review for word in word_tokenize(rev) if word.isalpha()])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_qZv5bBiGeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Frecuencia de cada palabra en el corpus\n",
        "word_freq = words_series.value_counts()\n",
        "word_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPQ8DVyTiGhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot de frecuencia vs ranking (la palabra mas frecuente tiene rank=1, la segunda tiene rank=2, etc..)\n",
        "plt.plot(range(len(word_freq)),word_freq)\n",
        "plt.xlabel('rank');plt.ylabel('freq');\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4-WgV_4gZ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ahora en eje logaritmico\n",
        "plt.plot(range(len(word_freq)),word_freq)\n",
        "plt.xlabel('rank');plt.ylabel('freq')\n",
        "plt.xscale('log'); plt.yscale('log');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGB_fDHjhvBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# palabras con 1 sola occurrencia\n",
        "sum(word_freq==1)/len(word_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id7N1_8ryjGq",
        "colab_type": "text"
      },
      "source": [
        "# Ejercicio 1\n",
        "## Armar un clasificador que identifique si una review de IMDB es positivo (1) o negativo (0)\n",
        "## comparar varios modelos y seleccionar el que produzca un mejor 10-fold CV o un mejor performance en el dev set\n",
        "## predecir en el test set y calcular la matriz de confusion, precision, recall y F-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2qzstytwb7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRIycFNqwb-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
        "\n",
        "(train_data, validation_data), test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=(train_validation_split, tfds.Split.TEST),\n",
        "    as_supervised=True)\n",
        "\n",
        "df_train = pd.DataFrame(list(tfds.as_numpy(train_data)),columns=['texto','clase'])\n",
        "df_dev = pd.DataFrame(list(tfds.as_numpy(validation_data)),columns=['texto','clase'])\n",
        "df_test = pd.DataFrame(list(tfds.as_numpy(test_data)),columns=['texto','clase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XSlXSN-wb4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikMYGyp2xQ16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def b2str(b):\n",
        "  return b.decode('utf8')\n",
        "\n",
        "df_dev['texto'] = df_dev['texto'].apply(b2str)\n",
        "df_train['texto'] = df_train['texto'].apply(b2str)\n",
        "df_test['texto'] = df_test['texto'].apply(b2str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUNe-mLxyjGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PEFKGLlyjGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}