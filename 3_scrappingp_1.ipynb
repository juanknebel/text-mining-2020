{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practica_guiada_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DSXQG1PUsCTk","colab_type":"text"},"source":["\n","# Una introducción a `BeatifulSoup`\n","### Germán Rosati (IDAES-UNSAM, CONICET, PIMSA)\n","\n","---\n","\n","\n","\n","La idea de esta práctica es introducir algunos elementos básicos para la utilización de la librería `BeautifulSoup`. Para ello, vamos a seguir algunos ejemplos del libro _Web Scraping with Pyhton_ de Ryan Mitchell.\n","\n","![alt text](https://cv02.twirpx.net/2545/2545056.jpg?t=20180604011432)"]},{"cell_type":"markdown","metadata":{"id":"oezcUkahr067","colab_type":"text"},"source":["## Scrapeando un sitio simple\n","---\n","Empecemos por un sitio no demasiado complejo. \n","\n","1. Importamos algunas funciones de la librería urllib.\n","2. `html = urlopen(\"http://pythonscraping.com/pages/page1.html\")` abrimos una conexión con la página web en cuestión)\n","3. `print(html.read())`, mostramos el contenido"]},{"cell_type":"code","metadata":{"id":"JyevXHGyrTrc","colab_type":"code","outputId":"46744cd3-b2fd-4776-d078-e7b6328bdb47","executionInfo":{"status":"ok","timestamp":1587507674118,"user_tz":180,"elapsed":709,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from urllib.request import urlopen\n","html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n","print(html.read())"],"execution_count":1,"outputs":[{"output_type":"stream","text":["b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yG0yfQm8ffiL","colab_type":"text"},"source":["Vemos que es un tanto incomprensible. Es una parva de código html. Para poder hacer un poco más itnerpretable (y utilizable) vamos a recurrir a la libreria [`BeatifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n","\n","\n","El objeto más comúnmente usado en BeautifulSoup es el `BeautifulSoup object`. Veamos cómo funciona."]},{"cell_type":"code","metadata":{"id":"5fFlOjMkrUXa","colab_type":"code","outputId":"b74a4b19-3353-4bd6-ec5b-7d5136a117f0","executionInfo":{"status":"ok","timestamp":1587507835975,"user_tz":180,"elapsed":775,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from bs4 import BeautifulSoup\n","html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n","bsObj = BeautifulSoup(html.read())\n","print(bsObj.h1)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["<h1>An Interesting Title</h1>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wQ7GkAKWrSRI","colab_type":"text"},"source":["Lo que hacemos, entonces, es importar la libreríía urlopen y llamar al método `html.read()` para obtener el contenido en html. Luego, transformamos ese objeto en `BeautifulSoup object` que lo transforma a esta estructura:\n","\n","```{html}\n","<html><head>...</head><body>...</body></html> => HTML\n","  <head><title>A Useful Page<title></head>  => HEAD\n","    <title>A Useful Page</title>  => TITLE\n","  <body><h1>An Int...</h1><div>Lorem ip...</div></body> => BODY\n","      <h1>An Interesting Title</h1>  => H1\n","      <div>Lorem Ipsum dolor...</div>  => DIV\n","```\n","\n","Notar que el tag `<h1>` extraído estaba anidado dos capas dentro del objeto bs.(html → body → h1). \n","\n","Sin embargo, cuando lo trajimos, simplemente lo llamamos:\n","\n","`bsObj.h1`\n","\n","En realidad, cualquiera de las siguientes llamadas producirá el mismo ouput:\n"]},{"cell_type":"code","metadata":{"id":"UbH1L7aK0jOA","colab_type":"code","outputId":"8d22372e-0cd8-465a-8c70-5ff31d788330","executionInfo":{"status":"ok","timestamp":1587507939545,"user_tz":180,"elapsed":904,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["bsObj.html.body.h1 ==  bsObj.body.h1 == bsObj.html.h1 == bsObj.h1"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"S2AohObH4C5M","colab_type":"text"},"source":["En líneas generales, casi cualquier información puede ser extraída de un archivo html a condición de que se encuentren cerca de algún tag que permita identificarlo."]},{"cell_type":"markdown","metadata":{"id":"KPSRfyN44Mmm","colab_type":"text"},"source":["## Parseando un html más complejo\n","---\n","\n","Poder mandarse de cabeza tan fácilmente en la estructura del html parece seductor. De hecho, uno podría pensar en generar sentencias cómo éstas:\n","\n","```python\n","bsObj.findAll(\"table\")[4].findAll(\"tr\")[2].find(\"td\").findAll(\"div\")[1].find(\"a\")\n","```\n","\n","- Poco legible\n","- Muy vulnerable al cambio en los sitios que se scrapean\n","\n","Veamos de qué forma podemos tratar de escribir algunos scrapers más prolijos y un poco más robustos.\n","\n","En general, casi el 100% de los websites contienen \"hojas de estilo\". Lo interesante es que esas hojas de estilo (codeadas en un estilo que se llama CSS) permiten taggear las diferentes partes del código html para darles un estilo diferente. Por ejemplo:\n","\n","```{html}\n","<span class=\"green\"></span>\n","<span class=\"red\"></span>\n","```\n","\n","Los scrapers pueden separar de forma muy simple esos dos tags basados en su atributo `class`. Por ejemplo, podríamos traer todo el texto escrito en rojo. \n","\n","Veamos ahora este [sitio](http://www.pythonscraping.com/pages/warandpeace.html)."]},{"cell_type":"code","metadata":{"id":"DIoZ-XAf5tUf","colab_type":"code","colab":{}},"source":["html = urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n","bsObj = BeautifulSoup(html)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQC67gpe8EnU","colab_type":"text"},"source":["Traigamos ahora, todas las palabras en verde."]},{"cell_type":"code","metadata":{"id":"HoGJxm1n7vbc","colab_type":"code","outputId":"319a2587-156f-4d73-cabb-cde024f5ae16","executionInfo":{"status":"ok","timestamp":1587508244794,"user_tz":180,"elapsed":1007,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":799}},"source":["nameList = bsObj.findAll(\"span\", {\"class\":\"green\"})\n","for name in nameList:\n","  print(name.get_text())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Anna\n","Pavlovna Scherer\n","Empress Marya\n","Fedorovna\n","Prince Vasili Kuragin\n","Anna Pavlovna\n","St. Petersburg\n","the prince\n","Anna Pavlovna\n","Anna Pavlovna\n","the prince\n","the prince\n","the prince\n","Prince Vasili\n","Anna Pavlovna\n","Anna Pavlovna\n","the prince\n","Wintzingerode\n","King of Prussia\n","le Vicomte de Mortemart\n","Montmorencys\n","Rohans\n","Abbe Morio\n","the Emperor\n","the prince\n","Prince Vasili\n","Dowager Empress Marya Fedorovna\n","the baron\n","Anna Pavlovna\n","the Empress\n","the Empress\n","Anna Pavlovna's\n","Her Majesty\n","Baron\n","Funke\n","The prince\n","Anna\n","Pavlovna\n","the Empress\n","The prince\n","Anatole\n","the prince\n","The prince\n","Anna\n","Pavlovna\n","Anna Pavlovna\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NrAzAln6j-k3","colab_type":"text"},"source":["## Mini práctica\n","---\n","Ahora traigan ustedes todas las palabras en rojo"]},{"cell_type":"code","metadata":{"id":"XAPSB8RnkR0v","colab_type":"code","colab":{}},"source":["# INSERTE EL CODIGO\n","\n","nameList = bsObj.findAll(\"span\", {\"class\":\"red\"})\n","\n","name_1 = []\n","for name in nameList:\n","  name_1.append(name.get_text())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JI3SDCMpo07Q","colab_type":"text"},"source":["### Una aclaración con `get_text()`\n","\n","- `.get_text()` elimina todos los tags del documento y devuelve un string que solamente contiene texto. Por ejemplo, si están trabajando con un bloque largo de texto que tiene links, imágenes, etc... todo eso desaparece y solo devuelve un bloque de texto. \n","- Es importante recordar que es mucho más fácil buscar cosas en el árbol del objeto `BeautifulSoup` que en texto plano.\n","- `.get_text()` debería ser usando cuando estén seguros de que ya no buscan nada más en el árbol."]},{"cell_type":"markdown","metadata":{"id":"lu2wA-dL8FFy","colab_type":"text"},"source":["### Quizás las dos funciones más útiles: `find() y find_all()`\n","---\n","\n","Esas son las dos funciones que, seguramente, más van a usar. Permiten filtrar de forma bastante simple pááginas html para encontrar listas de tags buscados o un solo tag, basados en sus atributos. Son muy similares en su sintaxis y en sus argumentos:\n","\n","```{python}\n","findAll(tag, attributes, recursive, text, limit, keywords)\n","find(tag, attributes, recursive, text, keywords)\n","```\n","En la mayoría de los casos van a necesitar solamente los dos primeros argumentos de la función: el tag buscado y sus atributos.\n","\n","- `tag`: pasamos un string con el nombre del tag que buscamos; por ejemplo, la línea siguiente va a traer todos los headers en el documento: `.findAll({\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"})`\n","- `attributes`: toma un diccionarii de atributos y matchea tags que contiene cualquiera de esos atributos; por ejemplo, `.findAll(\"span\", {\"class\":\"green\", \"class\":\"red\"})` va a devolver los tags con `class=green` y `class=red`\n","- `recursive`: es un booleano, que especifica cuál es la profundidad con la que se desea navegar el árbol. Si `recursive==True` la función busca hasta en los niveles más bajos tags que matcheen. Si se setea `recursive==False` solamente buscará en los tags de los nieles máás altos.\n","- `text`: no es muy usual y simplemente matchea basado en el texto contenido en el tag, en lugar de matchear los tags mismos. Por ejemplo, si quisiéramos ver cuántas veces aparece rodeado de tags el término \"the prince\" en la página anterior podríamos hacer los siguiente:"]},{"cell_type":"code","metadata":{"id":"nYUmP4Js9TzB","colab_type":"code","outputId":"44c367c3-6832-4b89-f4de-32b357602279","executionInfo":{"status":"ok","timestamp":1587509057671,"user_tz":180,"elapsed":897,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nameList = bsObj.findAll(text=\"the prince\")\n","print(len(nameList))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cqoPL1oR4KcD","colab_type":"text"},"source":["## Problemas de conectividad\n","---\n","Inevitablemente nos vamos a encontrar en algún momento con algún problema de conectividad. Vamos a querer descargar algún html y las cosas van a fallar. Más de una vez, hemos dejado corriendo un scraper toda la noche y resulta que el programa se cortó a la segunda iteración porque no logró bajar alguno de los links que pensábamos descargar.\n","\n","No obstante, hay algunas formas de evitar estos problemas. Volvamos a nuestra primera línea"]},{"cell_type":"code","metadata":{"id":"xdCETDNy4AaS","colab_type":"code","colab":{}},"source":["html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1M989i-8tVme","colab_type":"text"},"source":["Hay (básicamente) dos cosas que pueden fallar acá:\n","\n","- la página no está en el servidor\n","- el servidor no se encuentra\n","\n","En el primer caso, nos va a devolver un `HTTPError` Este HTTP error puede ser “404 Page Not Found,” “500 Internal Server Error,” etc. En todos los casos la función `urlopen` va a tirar `HTTPError`. Podemos manejar ese error de la \n","siguiente forma:\n","\n","```python\n","try:\n","  html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n","except HTTPError as e:\n","  print(e)\n","  #devuelve null, break, or hace alguna otra cosa\n","else:\n","  #el programa continua\n","```\n","\n","---\n","**Aclaración:** este uso de `try` y `except` forma parte de lo que se denomina manejo de excepciones (exception handling). Si bien queda fuera del alcance de esta clase pueden encontrar material en los siguientes links:\n","\n","- [documentación al respecto de Python](https://docs.python.org/3/tutorial/errors.html)\n","- [un lindo tutorial sobre el tema](https://realpython.com/python-exceptions/)\n","---"]},{"cell_type":"markdown","metadata":{"id":"UAOzupdUt1Wi","colab_type":"text"},"source":["Si aparece un HTTP error el programa ahora imprime el error y no ejecuta todo lo que está debajo del `else`.\n","\n","Si el problema es que no encuentra el servidor, por ejemplo, que el sitio estuviera mal escrito o caído, `urlopen` devuelve un `None`, algo así como un `NULL`, podemos, entonces, hacer un test para ver si lo que devolvió es `None`:\n","\n","\n","```python\n","if html is None:\n","  print(\"URL is not found\")\n","else:\n","# el programa sigue\n","```"]},{"cell_type":"markdown","metadata":{"id":"1E9EZOKMuB8Q","colab_type":"text"},"source":["Ahora bien, incluso si trajimos la página correctamente desde el server, todavía puede suceder que el sitio no esté formateado como lo esperamos. Suele ser una buena práctica chquear que el tag que buscamos existe... si no existe, `BeatifulSoup` devuelve un `None`."]},{"cell_type":"code","metadata":{"id":"Qi-QY7xvuQTT","colab_type":"code","outputId":"d27c6a4d-e2be-4973-90f1-9d157b68a8e0","executionInfo":{"status":"ok","timestamp":1587509654884,"user_tz":180,"elapsed":1367,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from urllib.request import urlopen\n","from urllib.error import HTTPError\n","from bs4 import BeautifulSoup\n","\n","def getTitle(url):\n","  try:\n","    html = urlopen(url)\n","  \n","  except HTTPError as e:\n","    return None\n","  try:\n","    bsObj = BeautifulSoup(html.read())\n","    title = bsObj.body.h1\n","  except AttributeError as e:\n","    return None\n","  return title\n","\n","title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n","if title == None:\n","  print(\"Title could not be found\")\n","else:\n","  print(title)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["<h1>An Interesting Title</h1>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvEWlKR_uTld","colab_type":"text"},"source":["Aquí creamos una función `getTitle` qye devuelve o bien el título de la página o un objeto `None` si hay algún problema. Dentro de la funciín se chequea si existe algún `HTTPError` y también encaspula en un solo `try` dos líneas de BeatifulSoup.\n","\n","Un `AttributeError` puede aparecer desde cualquiera de estas líneas (si el servidor no existiera, entonces el objeto html sería un `None` y html.read () arrojaría un `AttributeError`). \n","\n","Podríamos, de hecho, abarcar tantas líneas como quisiéramos dentro de `try`, o llamar a otra función por completo, lo que puede arrojar un `AttributeError` en cualquier momento.\n","\n","Al escribir scrapers, es importante pensar en el patrón general de su código\n","para manejar excepciones y hacerlo legible al mismo tiempo. También es probable\n","desea reutilizar mucho el código. Tener funciones genéricas como `getSiteHTML` y `getTitle` (con buen manejo de expeciones) es una buena práctica.\n"]},{"cell_type":"markdown","metadata":{"id":"ivef7M76qF12","colab_type":"text"},"source":["## Resumen\n","---\n","\n","Vimos algunas cuestiones básicas de scraping:\n","\n","- parseo de sitios html simples\n","- algunas funciones y métodos de `BeautifulSoup`\n","  - `urlopen()`\n","  - `BeautifulSoup()`\n","  - `find()` y `find_all()`\n","  - `.get_text()`\n","  - manejar excepciones con `try` y `except`\n","\n","Nos quedan unas cuántas cosas afuera en esta introducción. Solo por citar algunas:\n","\n","1. cómo navegar el árbol hacia arriba y hacia abajo (es decir los `children`, `siblings` y `parents`\n","2. Regex (o regular expressions) que, seguramente, verán en algún momento del curso\n","3. Como scrapear sitios que no son estáticos (por ejemplo que tienen formularios, etc., o paginado, etc.)\n","\n","Para los 3 pueden recurrir al libro  [Web Scraping with Pyhton de Ryan Mitchell](https://www.oreilly.com/library/view/web-scraping-with/9781491985564/) y/o a la documentación de [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n","\n","Para el último punto, además, les dejo dos paquetes que son óptimos para este tipo de tareas, aunque un poco más complejos de usar:\n","\n","- [Selenium](https://selenium-python.readthedocs.io/)\n","- [Scrapy](https://scrapy.org/)"]},{"cell_type":"markdown","metadata":{"id":"uv6LPkfQEyzP","colab_type":"text"},"source":["# APIS\n","---\n","Una consulta a una API (Applications Programming Interface) puede servir para  resolver muchos de los problemas que estuvimos viendo al momento de parsear un html. En efecto, proveen una interface conveniente y sumple para muchas aplicaciones. No importa si las aplicaciones fueron escritas por diferentes programadores, con diferentes arquitecturas... la idea de una API es servir como lingua franca entre diferentes pedazos de software para compartir información.\n","\n","## Cómo funciona una API\n","---\n","Si bien no son tan generalizadas como uno querría, suele haber una cantidad de APIS bastante grande y con diferentes tipos de información. Data de músicos y músoca (Spotify), de lugares (Google Places o OSM), de deportes (ESPN), de pobreza (Banco Mundial), etc.\n","\n","Las APIs son muy sumples de usar...\n","\n","Copien y peguen esta direccion en su navegador:\n","\n","> [https://freegeoip.live/json/50.78.253.58](https://freegeoip.live/json/50.78.253.58)\n","\n","Esto producirá la siguiente respuesta:\n","\n","```{json}\n","{\"ip\":\"50.78.253.58\",\"country_code\":\"US\",\"country_name\":\"United States\",\"region_code\":\"MA\",\"region_name\":\"Massachusetts\",\"city\":\"Fitchburg\",\"zip_code\":\"01420\",\"time_zone\":\"America/New_York\",\"latitude\":42.5781,\"longitude\":-71.8051,\"metro_code\":506}\n","```\n","\n","Es decir, navegamos a una dirección web en el navegador y ese navagador poroduce cierta información... que adeḿas (aunque no lo parezca) esta MUY BIEN FORMATEADA.\n","\n","¿Qué diferencia hay con los sitios web normales? Bastante poca. Usan el mismo protocolo (HTTP). Lo único ciertamente distintivo es que las APIs tienen una sintaxis extremadamente regulada y que las APIs tienden a presentar sus datos en formato JSON o XML y no en HTML.\n"]},{"cell_type":"markdown","metadata":{"id":"aEzIWRDPFrVB","colab_type":"text"},"source":["## Trayendo datos de pobreza\n","---\n","Veamos un ejemplo un poco más complejo. [PovcalNet](http://iresearch.worldbank.org/PovcalNet/home.aspx) es un proyecto del Banco Mundial. Se trata de una herrmienta que permite replicar los cálculos que el organismo realiza sobre la llamada \"Pobreza Absoluta\" para una gran grupo de paises en el mundo. No nos vamos a meter en detalles pero la cosa es que hay dos grandes formas de medir la pobreza: [la absoluta y la relativa](https://www.habitatforhumanity.org.uk/blog/2018/09/relative-absolute-poverty/). La pobreza absoluta define un cierto umbral de ingresos por medio del cual puede adquirirse una canasta mínima de bienes y servicios. Aquellos hogares que se encuentren por debajo de ese umbral serán clasificados omo \"pobres\". Ahora bien, las canastas varían notablemente entre países por diversos motivos (históricos, culturales, metodológicos) es por ello que el Banco Mundial definió el criterio de los \"2 dólares diarios\": cualquier persona que no llegue a ese ingreso es considerado como \"pobre extremo\".\n","\n","Lo interesante (entre otras cosas) de este proyecto es que no te hace bajar un mamotreto de datos sino que... tiene una API, bastante completa para trabajar. De hecho, podemos jugar con diferentes umbrales de pobreza absoluta. Si U$S nos parece demasiado bajo podemos utilizar otro umbral más alto y la API nos va a devolver cálculos de pobreza para dichos umbrales.\n","\n","\n","> **Dato anecdótico (y no tanto):** si consultan el sitio de PovcalNet van a encontrar varios documentos metodológicos. Además, van a encontar la \"historia\" de los famososo U$$2 diarios. Básicamente, se hizo un estudio sobre los 15 países con menor gasto de consumo personal per cápita del mundo (Malawi, Mali, Ethiopia, Sierra Leone, Niger, Uganda, Gambia, Rwanda, Guinea-Bissau, Tanzania, Tajikistan, Mozambique, Chad, Nepal y Ghana). Es decir, que se trata de un umbral bastante bajo...\n","\n","\n","Veamos una introducción a esta API. Vamos a trabajar sobre una partecita... la puntita del iceberg de este proyecto. [Acá](http://iresearch.worldbank.org/PovcalNet/docs/PovcalNet%20API.pdf) tienen la documentación completa de la API.\n","\n","*Nota: intenté calcular los mísmos índices para Argentina pero devuelve un conjunto vacío... raro*\n","\n","En líneas generales, esto es una consulta sintácticamente correcta a la API:\n","\n","> http://iresearch.worldbank.org/PovcalNet/PovcalNetAPI.ashx?C0=USA&PL0=2.5&Y0=2000&format=json'\n","\n","Tiene varias partes:\n","\n","- el protocolo: `http://`,\n","- el nombre del servidor: `iResearch.worldbank.org`,\n","- el nombre del sitio: `PovcalNet`,\n","- el \"handler\": `PovcalNetAPI.ashx`,\n","- el string de la query: `C0=USA&PL0=2.5&Y0=2000`,\n","- el formato de respuesta requerido: `format=json`.\n","\n","Centrémonos en los dos últimos;\n","\n","- `C0=USA` es el parámetro de país... en este caso, los Estados Unidos\n","- `PL0=2.5` refiere al umbral de los \"dólares diarios\". Quienes ganen menos (en este caso de U$S2.5 por día serán considerados como pobres.\n","- `Y0=2000` es el año de estimación\n","\n","Si hacen click en el enlacec verán que devuelve una respuesta en formato json con varios campos. Nos vamos a centrar en el que se llama `HC` que es simplemente el conteo en proprociones de la cantidad de personas que se encuentran por debajo del umbral. En este caso, son 0.007, es decir, menos del 1% de la poblacion en USA. \n","\n","\n","Hagamos una consulta más compleja. Primero, importamos las dos librerías que vamos a usar..."]},{"cell_type":"code","metadata":{"id":"Java4UoJ1sbw","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import requests"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmpJ3Z5AazkJ","colab_type":"text"},"source":["Vamos a hacer una consulta al sitio que nos devuelva\n","\n","- datos de USA `C0=USA`\n","- con línea de pobreza de U$S1.9 `pl=1.9`\n","- para los años 1991 y 2002 `Y0=1991,2000`\n","\n","Vamos a generar los parámetros en objetos y usar los [`f.string` de Python](https://realpython.com/python-f-strings/) para construir la url para hacer la consulta a la API:"]},{"cell_type":"code","metadata":{"id":"_9tk8bVbRBLZ","colab_type":"code","colab":{}},"source":["country='USA'\n","pl='1.9'\n","year='1991,2000'\n","form='json'\n","\n","url = f'http://iresearch.worldbank.org/PovcalNet/PovcalNetAPI.ashx?C0={country}&PL0={pl}&Y0={year}&format={form}'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gUIfesfla3cy","colab_type":"text"},"source":["Veamos si la URL quedó bien escrita:"]},{"cell_type":"code","metadata":{"id":"_6cJyfMjVueJ","colab_type":"code","outputId":"9da9f700-5134-491d-a99b-d76045010516","executionInfo":{"status":"ok","timestamp":1587249074161,"user_tz":180,"elapsed":876,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["url"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'http://iresearch.worldbank.org/PovcalNet/PovcalNetAPI.ashx?C0=USA&PL0=1.9&Y0=1991,2000&format=json'"]},"metadata":{"tags":[]},"execution_count":171}]},{"cell_type":"markdown","metadata":{"id":"WRRmz9NBArcq","colab_type":"text"},"source":["Luego, usamos el paquete requests para traer la url, pasarla a formato json y pandas para pasarla a un formato de dataframe:"]},{"cell_type":"code","metadata":{"id":"RiABTG-EVYCF","colab_type":"code","outputId":"9bd0dc62-a94d-4d4b-f1a4-bf620bb0bfbc","executionInfo":{"status":"ok","timestamp":1587249074163,"user_tz":180,"elapsed":687,"user":{"displayName":"German Rosati","photoUrl":"","userId":"01731410839870484616"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["req = requests.get(url).json()['PovResult']\n","req = pd.DataFrame.from_dict(req)\n","req"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>interpolated</th>\n","      <th>useMicroData</th>\n","      <th>CountryCode</th>\n","      <th>CountryName</th>\n","      <th>RegionCode</th>\n","      <th>CoverageType</th>\n","      <th>RequestYear</th>\n","      <th>DataYear</th>\n","      <th>DataType</th>\n","      <th>PPP</th>\n","      <th>PovertyLine</th>\n","      <th>Mean</th>\n","      <th>HC</th>\n","      <th>pg</th>\n","      <th>PovGapSqr</th>\n","      <th>Watts</th>\n","      <th>Gini</th>\n","      <th>Median</th>\n","      <th>pr.mld</th>\n","      <th>Polarization</th>\n","      <th>ReqYearPopulation</th>\n","      <th>SvyInfoID</th>\n","      <th>Decile</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>USA</td>\n","      <td>United States</td>\n","      <td>OHI</td>\n","      <td>3</td>\n","      <td>1991</td>\n","      <td>1991</td>\n","      <td>Income</td>\n","      <td>1</td>\n","      <td>1.9</td>\n","      <td>1623.643292</td>\n","      <td>0.004990</td>\n","      <td>0.003493</td>\n","      <td>0.002820</td>\n","      <td>0.010560</td>\n","      <td>0.3824</td>\n","      <td>1326</td>\n","      <td>0.2793</td>\n","      <td>-1</td>\n","      <td>252.98</td>\n","      <td>USA_N1991Y</td>\n","      <td>[0.01905, 0.03571, 0.04867, 0.06147, 0.07452, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>USA</td>\n","      <td>United States</td>\n","      <td>OHI</td>\n","      <td>3</td>\n","      <td>2000</td>\n","      <td>2000</td>\n","      <td>Income</td>\n","      <td>1</td>\n","      <td>1.9</td>\n","      <td>1906.026841</td>\n","      <td>0.007492</td>\n","      <td>0.005991</td>\n","      <td>0.005259</td>\n","      <td>0.009052</td>\n","      <td>0.4034</td>\n","      <td>1483</td>\n","      <td>0.3189</td>\n","      <td>-1</td>\n","      <td>282.16</td>\n","      <td>USA_N2000Y</td>\n","      <td>[0.01869, 0.03569, 0.04748, 0.05908, 0.07121, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   interpolated  ...                                             Decile\n","0         False  ...  [0.01905, 0.03571, 0.04867, 0.06147, 0.07452, ...\n","1         False  ...  [0.01869, 0.03569, 0.04748, 0.05908, 0.07121, ...\n","\n","[2 rows x 23 columns]"]},"metadata":{"tags":[]},"execution_count":172}]},{"cell_type":"markdown","metadata":{"id":"pUedxoh5V6yi","colab_type":"text"},"source":["## Mini actividad\n","---\n","Traer estimaciones de pobreza desde povcalnet para los Tanzania con un umbral de pobreza de U$S1.9 (los famosos \"dos dólares diarios\") para los años 1991 y 2000. ¿Cuál es la proporción de pobres según este criterio para cada año?\n","\n","Tip: el código de Tanzania en PovcalNet es TZA"]},{"cell_type":"code","metadata":{"id":"Ljb4rt7OV4w7","colab_type":"code","colab":{}},"source":["## INSERTE CODIGO AQUI\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhrGZJPzFc9o","colab_type":"text"},"source":["¿Qué pueden decir las proporiones de pobres extremos en Tanzania y USA?"]},{"cell_type":"markdown","metadata":{"id":"YGwDCEoc17IO","colab_type":"text"},"source":["## Autenticación\n","---\n","\n","Muchas veces las API requieren algún mecanismo de registro para poder usarlas. Esto puede ser por mera seguridad o para cobrar por su uso.\n","\n","La gran mayoría de los métodos de autenticación de API generalmente usan algún tipo de token que se pasa al servidor web con cada llamada API realizada. El token es proporcionado al usuario cuando el usuario se registra y es un elemento permanente de la llamadas del usuario (generalmente en aplicaciones de baja seguridad), o puede cambiar con frecuencia, y se recupera del servidor utilizando una combinación de nombre de usuario y contraseña.\n","\n","Por ejemplo, para hacer una consulta al Sistema Integrado de Información Sanitaria Argentina del Ministerio de Salud sobre los establecimientos de salud, deberíamos escribir algo como esto:\n","\n","`https://sisa.msal.gov.ar/sisa/services/rest/establecimiento/buscar?provincia=1&redEstablecimiento=5Post:{\"usuario\":\"jperez\",\"clave\":\"xxxx\"}`\n","\n","Usando urllib podríamos tratar de hacer una consulta:\n","\n","```python\n","usuario = \"<tu user>\"\n","clave = \"<tu clave>\"\n","webRequest = urllib.request.Request(\"http://myapi.com\", headers={\"usuario\":token, \"clave\": clave})\n","html = urlopen(webRequest)\n","```\n","\n","De cualquier forma, en este tutorial vamos a usar una librería de Python que nos va a facilitar el manejo de la autenticación."]},{"cell_type":"code","metadata":{"id":"3EIxHPPriTll","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}