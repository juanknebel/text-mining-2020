{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, nmf, LdaMulticore\n",
    "import spacy\n",
    "# you need to run python -m spacy download en\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim\n",
    "from wordcloud import WordCloud\n",
    "pyLDAvis.enable_notebook()\n",
    "import warnings, sys, traceback, itertools, collections, logging\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the topic modelling discover ...\")\n",
    "t_total = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuraciones del logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = '%(asctime)-15s - %(filename)s:%(lineno)s - %(funcName)20s() \\n%(message)s'\n",
    "logging.basicConfig(filename='./topic_modelling.log', filemode='w', level=logging.ERROR, format=FORMAT)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "bad_ids=['like','say','remember','dream','think','know','could','go','would','want','tell','thing','start','come','back','look','people','ask','seem','talk','make','take', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuraciones generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../data'\n",
    "from_file = False\n",
    "experiment = 11\n",
    "number_of_topics_list = [90, 95, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(f'{data_directory}/dreamers_summary.csv', sep='|')\n",
    "dream = pd.read_csv(f'{data_directory}/dreams_clean.csv', sep=';')\n",
    "# Borro aquellos sue√±os que no tienen palabras y aquellos en aleman que son los del grupo con id 18, 26 y 27\n",
    "dream = dream.dropna(axis=0, subset=['words']).drop(dream.loc[dream['group_id'].isin([18, 26, 27, 79, 80])].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dream, summary, left_on='group_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['description'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences, word_min_len=2):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True, min_len=word_min_len))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words, bigram_mod, trigram_mod, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'], word_min_len=2, bad_ids = []):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc), min_len=word_min_len) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    #texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), min_len=word_min_len) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(model, corpus, texts, n_words):\n",
    "    dominant_topics_df = pd.DataFrame()\n",
    "    for topic_distribution in model[corpus]:\n",
    "        sorted_topics = sorted(topic_distribution[0], key=lambda x: (x[1]), reverse=True)\n",
    "        try:\n",
    "            topic_number, topic_prob = sorted_topics[0]\n",
    "            topic_first_n_words = \", \".join([word for word, prob in model.show_topic(topic_number, topn=n_words)])\n",
    "            dominant_topics_df = dominant_topics_df.append(pd.Series([topic_number, topic_prob, topic_first_n_words]), ignore_index=True)\n",
    "        except:\n",
    "            dominant_topics_df = dominant_topics_df.append(pd.Series([None, None, None]), ignore_index=True)\n",
    "            logger.error('%s', traceback.format_exc())\n",
    "\n",
    "    topics_df = pd.concat([dominant_topics_df, pd.Series(texts)], axis=1)\n",
    "    topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Original_Text']\n",
    "    \n",
    "    return(topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Yielding the words ...\")\n",
    "t0 = time()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(f\"... done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating bigram and trigram ...\")\n",
    "t0 = time()\n",
    "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=0.5, scoring='npmi') # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "print(f\"... done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre processing words: remove stopwords, form Bigrams and Lemmatization ...\")\n",
    "t0 = time()\n",
    "data_ready = process_words(data_words, stop_words, bigram_mod, None, word_min_len=3, bad_ids=bad_ids)\n",
    "print(f\"... done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dictionary and corpus ...\")\n",
    "t0 = time()\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "print(f\"Length of the dictionary is {len(id2word)}\")\n",
    "\n",
    "print(\"Filtering the extremes no_below=10, no_above=0.5 ...\")\n",
    "id2word.filter_extremes(no_below=10, no_above=0.5)\n",
    "print(f\"Length of the filter dictionary is {len(id2word)}\")\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "print(f\"... done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not from_file:\n",
    "    print(\"Applying LDA topic modelling ...\")\n",
    "    t0 = time()\n",
    "    lda_model_dict = {}\n",
    "    for num_topics in number_of_topics_list:\n",
    "        print(f\"Looking for {num_topics} topics ...\")\n",
    "        t1 = time()\n",
    "        lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                    workers=5,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=num_topics, \n",
    "                                                    random_state=100,\n",
    "                                                    chunksize=1000,\n",
    "                                                    iterations=1000,\n",
    "                                                    passes=10,\n",
    "                                                    per_word_topics=True)\n",
    "        lda_model_dict[num_topics] = lda_model\n",
    "        print(f\"... done in {time() - t1}s for {num_topics} topics.\")\n",
    "\n",
    "    print(f\"... all done in {time() - t0}s.\")\n",
    "\n",
    "    for num_topics, model in lda_model_dict.items():\n",
    "        model.save(f\"{data_directory}/models/lda_topics_{num_topics}_exp_{experiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_file:\n",
    "    num_topics = 100\n",
    "    experiment = 3\n",
    "    lda_model_dict = {}\n",
    "    lda_model_dict[num_topics] = LdaModel.load(f\"{data_directory}/models/lda_topics_{num_topics}_exp_{experiment}\")\n",
    "    print(\"Load succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating the distribution between topics and dreams ...\")\n",
    "t0 = time()\n",
    "for num_topics, lda_model in lda_model_dict.items():\n",
    "    t1 = time()\n",
    "    print(f\"Formating topics, corpus and saving them for lda model with {num_topics} topics ...\")\n",
    "    df_topic_sents_keywords = format_topics_sentences(model=lda_model, corpus=corpus, texts=data, n_words=10)\n",
    "    df_topic_sents_keywords.to_csv(f\"{data_directory}/lda_topic_example_{num_topics}_exp_{experiment}.csv\", sep=\";\", index=False)\n",
    "    print(f\"... done in {time() - t1}s.\")\n",
    "print(f\"... all done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating the coherence values ...\")\n",
    "t0 = time()\n",
    "for num_topics, model in lda_model_dict.items():\n",
    "    topics = [[word for word, prob in topic] for topicid, topic in model.show_topics(num_topics, formatted=False)]\n",
    "    cm = CoherenceModel(topics=topics, texts=data_ready, dictionary=id2word, coherence='c_v',topn=10)\n",
    "    #print(f\"Coherende per topic {cm.get_coherence_per_topic()}\")\n",
    "    print(f\"Coherence total for {num_topics} topics {cm.get_coherence()}\")\n",
    "print(f\"... all done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"... all the work was done in {time() - t_total}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = ','.join(data_ready[2])# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')# Generate a word cloud\n",
    "wordcloud.generate(long_string)# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp = LdaModel.load(f\"{data_directory}/models/lda_topics_30_exp_6\")\n",
    "topics = [[word for word, prob in topic] for topicid, topic in model_temp.show_topics(30, formatted=False)]\n",
    "cm = CoherenceModel(topics=topics, texts=data_ready, dictionary=id2word, coherence='c_v',topn=10)\n",
    "print(f\"Coherence total for {num_topics} topics {cm.get_coherence()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vietnam y Phil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vietnam = df.loc[df['group'].isin(['Vietnam Vet: 1970-2008 war dreams', 'Vietnam Vet: 2015 dreams', 'Vietnam Vet: 2016-17 dreams'])]\n",
    "df_phil = df.loc[df['group'].isin(['Phil 1: teens', 'Phil 2: late 20s', 'Phil 3: retirement'])]\n",
    "df_pegasus = df.loc[df['group'].isin(['Pegasus: a factory worker'])]\n",
    "df_norman = df.loc[df['group'].isin(['Norman: a child molester'])]\n",
    "\n",
    "print(f\"Se cuenta con {len(df_vietnam)} sue√±os de Vietnam. El corpus tiene {int(df_vietnam['words'].sum())} palabras.\")\n",
    "print(f\"Se cuenta con {len(df_phil)} sue√±os de Phil, nuestro conjunto de control. El corpus tiene {int(df_phil['words'].sum())} palabras.\")\n",
    "print(f\"Se cuenta con {len(df_pegasus)} sue√±os de Pegasus. El corpus tiene {int(df_pegasus['words'].sum())} palabras.\")\n",
    "print(f\"Se cuenta con {len(df_norman)} sue√±os de Norman, nuestro conjunto de control. El corpus tiene {int(df_norman['words'].sum())} palabras.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF solo con hasta 15 topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics_list_nmf = [10,15]\n",
    "print(\"Applying NMF topic modelling ...\")\n",
    "t0 = time()\n",
    "nmf_model_dict = {}\n",
    "for num_topics in number_of_topics_list_nmf:\n",
    "    print(f\"Looking for {num_topics} topics ...\")\n",
    "    t1 = time()\n",
    "    nmf_model = gensim.models.nmf.Nmf(corpus=corpus,\n",
    "                                id2word=id2word,\n",
    "                                num_topics=num_topics, \n",
    "                                random_state=100,\n",
    "                                eval_every=5,\n",
    "                                chunksize=10,\n",
    "                                passes=10,\n",
    "                                kappa=0.1)\n",
    "    nmf_model_dict[num_topics] = nmf_model\n",
    "    print(f\"... done in {time() - t1}s for {num_topics} topics.\")\n",
    "\n",
    "print(f\"... all done in {time() - t0}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topics = []\n",
    "for topic_distribution in nmf_model_dict[10][corpus[:]]:\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: (x[1]), reverse=True)\n",
    "    try:\n",
    "        list_of_topics.append(sorted_topics[0][0])\n",
    "    except:\n",
    "        list_of_topics.append(-1)\n",
    "\n",
    "topics_df = pd.DataFrame(list_of_topics, columns=['topic_number'])\n",
    "print(\"--------------------------------------------\")\n",
    "topics_df['topic_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[word for word, prob in topic] for topicid, topic in nmf.show_topics(formatted=False)]\n",
    "#cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, coherence='u_mass')\n",
    "cm = CoherenceModel(topics=topics, texts=data_ready, dictionary=id2word, coherence='c_npmi')\n",
    "print(f\"Coherende per topic {cm.get_coherence_per_topic()}\")\n",
    "print(f\"Coherence total {cm.get_coherence()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freqs = collections.Counter(itertools.chain.from_iterable(data_ready))\n",
    "doc_freqs = collections.Counter(itertools.chain.from_iterable(set(doc) for doc in data_ready))\n",
    "missing = [token for token in corpus_freqs if corpus_freqs[token] == 10 and token not in id2word.id2token]\n",
    "[(token, corpus_freqs[token], doc_freqs[token]) for token in missing]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}